# Speeding_NVLM_Decoder_Inference_Computation
Using Different framework and method to speed up the NVLM inference
NYCU HPC Team 團隊來自 陽明交通大學，將 NVLM 加速37.81倍！！
* Team Members: Chu-Siang Tseng, Shun-Yu Yang, Cheng-Wei Lin, Chen-Kai Chang, Zong-Hua Wu, Jia-Hui Shen
* NVIDIA Mentors: Shijie Wang.

在NVLM推理上的加速研究是一個極具潛力的領域。NVLM（NVIDIA Vision Language Model）是一種尖端的多模態大模型，其結合了文本和圖像的處理能力，並在視覺-語言任務上達到了最先進的成果。加速推理過程能夠顯著降低計算資源和時間的需求，使這些強大的模型能夠更快地應用於現實情境，例如圖像識別、文檔分析、以及視覺問答等應用。
這個領域的重要性在於多模態模型的廣泛應用潛力，尤其是那些需要即時回應的場景，如自動駕駛、醫療診斷或智能助理系統等。加速NVLM的推理能讓模型更加高效和實時，進一步推動其在工業界和日常生活中的普及和應用。
目前的加速研究已經顯示出明顯的成果，透過優化模型架構（例如混合架構和動態高解析度影像處理），推理速度提升了，並且能在保持甚至提升模型準確性的同時減少計算成本。這樣的加速帶來了多方面的影響，不僅降低了硬體資源的消耗，還使得這些多模態應用更為普及，進而推動整個AI領域向著更高效、更智能的方向發展。

我們也因為LLM,LMM的趨勢 選擇這主題來當作這次Hackthon 加速主題, 其中我們試了多種框架最後由 Mentor 的版本 加速了41.72倍推論時間

更多資訊請看：[https://github.com/chu-siang/Speeding_NVLM_Decoder_Inference_Computation](https://github.com/nqobu/nvidia/tree/main/20241204 )
